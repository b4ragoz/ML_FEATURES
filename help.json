{
  "import": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom datetime import datetime\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest, RFE, f_classif\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc",
  "obrabotka": "data = pd.read_csv('card_transdata.csv')\ndata.isna().sum()\ndata['repeat_retailer'] = data['repeat_retailer'].astype(int)\ndata['IMDB Votes'] = data['IMDB Votes'].str.replace(',', '').astype(int)\ndata.duplicated().sum()\ndata_sorted = data.sort_values(by='distance_from_home')\nprint(data_sorted.tail(10))\ndel data['Unnamed: 0']\nfor row in columns_to_unknown:\n    data[row] = data[row].fillna('unknown')\nfor row in columns_to_median:\n    data[row] = data[row].fillna(data[row].median())\ndata.dropna(subset=['Original Release Date','Streaming Release Date'], inplace=True)\ndata.describe()[['Year of Release','Movie Time']],",
  "teplovayaKarta": "correlation_matrix= data.corr()\nplt.figure(figsize=(20,10))\nsns.heatmap(correlation_matrix, annot=True, cmap='YlGnBu', fmt='.1f')\nplt.show(),",
  "vibrosi": "outlier = data[['distance_from_home', 'distance_from_last_transaction']]\n Q1 = outlier.quantile(0.01)\n Q3 = outlier.quantile(0.99)\n IQR = Q3-Q1\n data_filtered = outlier[~((outlier < (Q1 - 1.5 * IQR)) |(outlier > (Q3 + 1.5 * IQR))).any(axis=1)]\n index_list = list(data_filtered.index.values)\n data_filtered = data[data.index.isin(index_list)],",
  "matricaOshibok": "sns.heatmap(confusion_matrix(y_test, y_knn_pred), annot=True, fmt='d', cmap='Blues', cbar=False)\n plt.xlabel('Предсказанные классы')\n plt.ylabel('Фактические классы')\n plt.title('Матрица ошибок')\n plt.show(),",
  "disbalans": "data_filtered.groupby('fraud').size().plot(kind='pie',y='v1',label='Type',autopct='%1.1f%%')\n tran=data_filtered[data_filtered['fraud']==1]\n not_tran=data_filtered[data_filtered['fraud']==0]\n from sklearn.utils import resample\n not_downsample=resample(not_tran,replace=True,n_samples=len(tran),random_state=42)\n data_ds=pd.concat([tran,not_downsample]),",
  "testITrainPlusSkalirovaniye": "y = data_ds['fraud']\n X = data_ds.drop(['fraud'], axis=1)\n X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n scaler = StandardScaler()\n df[['one', 'four']] = scaler.fit_transform(df[['one', 'four']]),",
  "oneHot": "data=pd.get_dummies(data)",
  "ponizeniyeRaznernosti": "selector_cl = VarianceThreshold(threshold=0.1)\n X_cl_reduced = selector_cl.fit_transform(X_cl)\n support_cl = selector_cl.get_support()\n X_cl_reduced_df = pd.DataFrame(X_cl_reduced, columns=X_cl.columns[support_cl])\n selector_cl_kbest = SelectKBest(f_classif, k=4)\n X_cl_kbest = selector_cl_kbest.fit_transform(X_cl, y_cl)\n support_cl_kbest = selector_cl_kbest.get_support()\n X_cl_kbest_df = pd.DataFrame(X_cl_kbest, columns=X_cl.columns[support_cl_kbest]),",
  "metriki": "y_log_pred = grid_search_log.predict(X_test_scaled)\n print('Accuracy:', accuracy_score(y_test, y_log_pred))\n print('Precision:', precision_score(y_test, y_log_pred))\n print('Recall:', recall_score(y_test, y_log_pred))\n print('F1 Score:', f1_score(y_test, y_log_pred))\n y_log_proba = grid_search_log.predict_proba(X_test_scaled)[:, 1]\n print('ROC AUC Score:', roc_auc_score(y_test, y_log_proba)),",
  "kNN": "knn = KNeighborsClassifier()\nparam_grid_knn = {'n_neighbors': [3, 5, 11, 19], 'weights': ['uniform', 'distance'], 'metric': ['euclidean', 'manhattan']}\ngrid_search = GridSearchCV(knn, param_grid_knn, cv=5, scoring='accuracy')\ngrid_search.fit(X_train_scaled, y_train)\nprint('Best Parameters: ', grid_search.best_params_)\ny_knn_pred = grid_search.predict(X_test_scaled),",
  "logisticRegression": "logreg = LogisticRegression()\nparam_grid_lr = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Сила регуляризации, меньше значение - сильнее регуляризация\n    'penalty': ['l1', 'l2'],  # Тип регуляризации\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']  # Алгоритм оптимизации\n}\ngrid_search_log = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\ngrid_search_log.fit(X_train_scaled, y_train)\nprint('Best Parameters: ', grid_search_log.best_params_)\ny_log_pred = grid_search_log.predict(X_test_scaled),",
  "sVM": "from sklearn.svm import SVC\n svm = SVC()\n param_grid_svm = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf', 'poly', 'sigmoid']}\n grid_search_log = GridSearchCV(svm, param_grid_svm, cv=5, scoring='accuracy')\n grid_search_log.fit(X_train_scaled, y_train)\n print('Best Parameters:', grid_search_log.best_params_)\n y_log_pred = grid_search_log.predict(X_test_scaled),",
  "decisionAndRandomTree": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor\ndt_classifier = DecisionTreeClassifier()\nparam_grid_dt = {'criterion': ['gini', 'entropy'], 'max_depth': [None, 10, 20, 30, 40, 50], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\ngrid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid_dt, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\ny_pred=grid_search.predict(X_test)\ntest_accuracy = grid_search.score(X_test, y_test)\nprint('Лучшие гиперпараметры:', grid_search.best_params_)\nprint('Точность на тестовых данных:', test_accuracy)\nrf = RandomForestClassifier()\nparam_grid = {'n_estimators': [10, 50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [None, 5, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X, y),",
  "catboostXGBOOSTLightBM": "!pip install catboost\nfrom catboost import CatBoostClassifier\nparam_grid_gb = {\n 'n_estimators': [100, 200, 300], # Number of boosting stages\n 'learning_rate': [0.1, 0.01, 0.001], # Learning rate\n 'max_depth': [3, 4, 5], # Maximum depth of individual regression estimators\n 'min_samples_split': [2, 4, 6], # Minimum number of samples required to split\n 'min_samples_leaf': [1, 2, 4] # Minimum number of samples at leaf node\n}\ncat_grid = GridSearchCV(CatBoostClassifier(), params, cv=5, , scoring='accuracy')\ncat_grid.fit(X_train, y_train)\nprint('Best parameters for CatBoost:', cat_grid.best_params_)\ny_pred_classifier = cat_grid.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred_classifier)\nprint(f'Accuracy: {accuracy}')\nprint(classification_report(y_test, y_pred_classifier))\n# Confusion matrix visualization\nsns.heatmap(confusion_matrix(y_test, y_pred_classifier), annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.xlabel('Predicted classes')\nplt.ylabel('Actual classes')\nplt.title('Confusion Matrix')\nplt.show()\nimport xgboost as xgb\nxgb_grid = GridSearchCV(xgb.XGBClassifier(), params, cv=5, , scoring='accuracy')\nimport lightgbm as lgb\nlgb_classifier = GridSearchCV(lgb.LGBMClassifier(), params, cv=5, , scoring='accuracy'),",
  "boxPlot": "sns.boxplot(y='distance_from_last_transaction',data=data)",
  "streamlit": "import streamlit as st\nfrom tensorflow.keras.models import load_model\n\nmodel_save_path = 'streamlit-models/'\ncsgo_data = pd.read_csv('csgo.csv')\nX = csgo_data.drop('bomb_planted_True', axis=1)\ny = csgo_data['bomb_planted_True']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndef load_models():\n    model_ml1 = pickle.load(open(model_save_path + 'model_ml1.pkl', 'rb'))\n    model_ml4 = pickle.load(open(model_save_path + 'model_ml4.pkl', 'rb'))\n    model_ml5 = pickle.load(open(model_save_path + 'model_ml5.pkl', 'rb'))\n    model_ml3 = XGBClassifier()\n    model_ml3.load_model(model_save_path + 'model_ml3.json')\n    model_ml6 = load_model(model_save_path + 'model_ml6.h5')\n    model_ml2 = pickle.load(open(model_save_path + 'kmeans_model.pkl', 'rb'))\n    return model_ml1, model_ml3, model_ml4, model_ml5, model_ml6, model_ml2\n\nst.markdown('''\n<style>\n.sidebar .sidebar-content {\n    background-color: #f1f3f6;\n}\nh1 {\n    color: #0e1117;\n}\n</style>\n''', unsafe_allow_html=True)\n\nst.sidebar.image('kawasaki.png', width=100)\nst.sidebar.title('Навигация')\npage = st.sidebar.radio(\n    'Выберите страницу:',\n    ('Информация о разработчике', 'Информация о наборе данных', 'Визуализации данных', 'Предсказание модели ML')\n)\n\ncsgo_data = pd.read_csv('csgo.csv')\n\ndef page_dataset_info():\n    st.title('Информация о наборе данных')\n\n    st.markdown('''\n    ## Описание Датасета CS:GO\n    **Файл датасета:** 'csgo.csv'\n\n    **Описание:**\n    Данный датасет содержит статистическую информацию о матчах в популярной компьютерной игре Counter-Strike: Global Offensive (CS:GO). Включает следующие столбцы:\n\n    - 'index': Индекс записи.\n    - 'time_left': Время до конца раунда.\n    - 'ct_score': Счёт команды контр-террористов.\n    - 't_score': Счёт команды террористов.\n    - 'ct_health': Общее здоровье команды контр-террористов.\n    - 't_health': Общее здоровье команды террористов.\n    - 'ct_armor': Уровень брони команды контр-террористов.\n    - 't_armor': Уровень брони команды террористов.\n    - 'ct_money': Деньги команды контр-террористов.\n    - 't_money': Деньги команды террористов.\n    - 'ct_helmets': Шлемы команды контр-террористов.\n    - 't_helmets': Шлемы команды террористов.\n    - 'ct_defuse_kits': Комплекты для обезвреживания бомбы у CT.\n    - 'ct_players_alive': Живые игроки команды контр-террористов.\n    - 't_players_alive': Живые игроки команды террористов.\n    - 'bomb_planted_True': Индикатор заложенной бомбы.\n\n    **Особенности предобработки данных:**\n    - Удаление лишних столбцов, например, 'index'.\n    - Обработка пропущенных значений.\n    - Нормализация числовых данных для улучшения производительности моделей.\n    - Кодирование категориальных переменных.\n    ''')\n\n\ndef page_ml_prediction():\n    st.title('Предсказания моделей машинного обучения')\n\n    uploaded_file = st.file_uploader('Загрузите ваш CSV файл', type='csv')\n\n    if uploaded_file is None:\n        st.subheader('Введите данные для предсказания:')\n\n        input_data = {}\n        feature_names = ['index', 'time_left', 'ct_score', 't_score', 'ct_health', 't_health', 'ct_armor', 't_armor', 'ct_money', 't_money', 'ct_helmets', 't_helmets', 'ct_defuse_kits', 'ct_players_alive', 't_players_alive']\n        for feature in feature_names:\n            input_data[feature] = st.number_input(f'{feature}', min_value=0.0, max_value=100000.0, value=50.0)\n\n        if st.button('Сделать предсказание'):\n            model_ml1, model_ml3, model_ml4, model_ml5, model_ml6, model_ml2 = load_models()\n            input_df = pd.DataFrame([input_data])\n            st.write('Входные данные:', input_df)\n            scaler = StandardScaler().fit(X_train)\n            scaled_input = scaler.transform(input_df)\n            prediction_ml1 = model_ml1.predict(scaled_input)\n            st.success(f'Результат предсказания LogisticRegression: {prediction_ml1[0]}')\n    else:\n        try:\n            model_ml1 = pickle.load(open(model_save_path + 'model_ml1.pkl', 'rb'))\n            predictions_ml1 = model_ml1.predict(X_test)\n            accuracy_ml1 = accuracy_score(y_test, predictions_ml1)\n            st.success(f'Точность LogisticRegression: {accuracy_ml1}')\n        except Exception as e:\n            st.error(f'Произошла ошибка при чтении файла: {e}')\n\nif page == 'Информация о разработчике':\n    page_developer_info()\nelif page == 'Информация о наборе данных':\n    page_dataset_info()\nelif page == 'Визуализации данных':\n    page_data_visualization()\nelif page == 'Предсказание модели ML':\n    page_ml_prediction(),"
}