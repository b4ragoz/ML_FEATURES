# -*- coding: utf-8 -*-
"""Копия блокнота "lab3_pushkarev_FINALLY.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D5b1wQPQFsaloGFJyImy4s4cnhv7Bf7V

# Лабароторная работа 3. Обучение с учителем. Задача Регрессии.

# Задача регрессии

Загрузка данных.
"""

import pandas as pd

data = pd.read_csv('moldova_cars_task.csv')
data
data.info()

"""Предобработка

Подсчет пустых значений
"""

#кол во пропущенных значений
data.isna().sum()

columns_to_unknown = ['Model'] #в данных столбцах пустые ячейки будут названы unknown
columns_to_median = ['Year', 'Distance'] # в данных столбцах ячейки будут заполнены медианой значений

for row in columns_to_unknown:
    data[row] = data[row].fillna('unknown')

for row in columns_to_median:
  data[row]=data[row].fillna(data[row].median())

data.dropna(subset=['Transmission', 'Style'],inplace=True) # там где нельзя предсказать данные, строки будут удалены

data.isna().sum()

"""Все типы данных установлены верно, поэтому изменять их тип не будем.

**EDA**
"""

data.head(10)

"""Составим тепловую карту, чтобы визуализировать корреляцию между признаками в датасете."""

import seaborn as sns
import matplotlib.pyplot as plt


correlation_matrix = data.corr()


plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Тепловая карта корреляции признаков')
plt.show()

"""- Year
- Distance
- Engine_capacity(cm3)
- Price(euro)

На эти перечисленные параметры будет обращено особое внимание с помощью гистограмм. Здесь выбросы будут влиять на форму и читаемость гистограмм.
"""

data.describe()[['Year', 'Distance', 'Engine_capacity(cm3)', 'Price(euro)']]

columns = ['Year', 'Distance', 'Engine_capacity(cm3)', 'Price(euro)']

for column in columns:
    data.hist(column)

# Визуализация выбросов с помощью boxplots

plt.figure(figsize=(20, 12))

for idx, col in enumerate(['Year', 'Distance', 'Engine_capacity(cm3)', 'Price(euro)'], start=1):
    plt.subplot(2, 2, idx)
    sns.boxplot(x=data[col])
    plt.title(f'Boxplot для {col}')
    plt.xlabel(col)

plt.tight_layout()
plt.show()

"""Избавимся от выбросов"""

# Функция для очистки данных от выбросов на основе IQR
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Очистка данных от выбросов для каждого числового столбца
cleaned_data = data.copy()
for col in ['Year', 'Distance', 'Engine_capacity(cm3)', 'Price(euro)']:
    cleaned_data = remove_outliers(cleaned_data, col)

# Сравним размеры исходного и очищенного датасетов
original_size = data.shape[0]
cleaned_size = cleaned_data.shape[0]
original_size, cleaned_size

# Визуализация выбросов с помощью boxplots

plt.figure(figsize=(20, 12))

for idx, col in enumerate(['Year', 'Distance', 'Engine_capacity(cm3)', 'Price(euro)'], start=1):
    plt.subplot(2, 2, idx)
    sns.boxplot(x=cleaned_data[col])
    plt.title(f'Boxplot для {col}')
    plt.xlabel(col)

plt.tight_layout()
plt.show()

"""**Выделение целевой переменной и предикторов**"""

cleaned_data.head()
cleaned_data.to_csv (r'cleaned_cars.csv', index= False )

# Создание X и y на основе очищенного датасета
y = cleaned_data["Price(euro)"]
X = cleaned_data.drop(["Price(euro)", 'Make', 'Model', 'Style'], axis=1)

# Замена значений в столбце Transmission
X['Transmission'] = X['Transmission'].replace({'Automatic': 1, 'Manual': 0})
X['Fuel_type'] = X['Fuel_type'].replace({'Diesel': 1, 'Petrol': 2, 'Metan/Propan': 3, 'Hybrid': 4, 'Plug-in Hybrid': 5, 'Electric': 6})



X.head()  # Вывод первых строк обновленного набора признаков

y

"""**Разделение данных на обучающую и тестовую выборки**

Итак, данные предобработаны, целевой признак выделен, имеются обучающая и тестовая выборка. Мы готовы приступить к обучению.

Реализуем для начала простую регрессию
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

simple_X = cleaned_data[['Year']]
simple_y = cleaned_data['Price(euro)']


# Разделение данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(simple_X, simple_y, test_size=0.2, random_state=42)


# Обучение модели
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Предсказание на тестовой выборке
y_pred = regressor.predict(X_test)

# Оценка модели
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

mse, r2

"""Вот результаты простой линейной регрессии:

    Среднеквадратическая ошибка (MSE): 13148277.32648257
    Коэффициент детерминации R^2: 0.5078512817250964

Коэффициент детерминации R^2 указывает на то, что примерно 51% вариации цены можно объяснить годом выпуска автомобиля.

**другой признак**

**Линейная регрессия (Linear Regression). Постановка задачи.**

Теперь проведем ту же процедуру, но с большим количеством предикторов.
"""

from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from math import sqrt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix


# Разделение данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Обучение модели
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Предсказание на тестовой выборке
y_pred = regressor.predict(X_test)

print(f'MAE: {mean_absolute_error(y_test, y_pred)}')
print(f'MSE: {mean_squared_error(y_test, y_pred)}')
print(f'RMSE: {sqrt(mean_squared_error(y_test, y_pred))}')
print(f'MAPE: {sqrt(mean_absolute_percentage_error(y_pred, y_pred))}')
print(f'R^2: {regressor.score(X_test, y_test)}')

"""После того как мы началали давать для обучения больше предикторов, все показатели занчительно выросли. Показатель R^2 = 67%, это значит что успешность предсказания цены автомобиля равна 67%. Также показатель MSE уменьшился, что значит количество ошибок сократилось.

Теперь попробуем применить регуляризации.
"""

from sklearn.linear_model import Lasso, Ridge
from sklearn.metrics import r2_score

#L1
# Создание и обучение Lasso модели
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# Предсказание с использованием Lasso модели
y_predict_lasso = lasso.predict(X_test)

print(f'MAE: {mean_absolute_error(y_test, y_predict_lasso)}')
print(f'MSE: {mean_squared_error(y_test, y_predict_lasso)}')
print(f'RMSE: {sqrt(mean_squared_error(y_test, y_predict_lasso))}')
print(f'MAPE: {sqrt(mean_absolute_percentage_error(y_predict_lasso, y_predict_lasso))}')
print(f'R^2: {lasso.score(X_test, y_test)}')
print('--------------')


#L2
# Создание и обучение Ridge модели
ridge = Ridge(alpha=0.1)
ridge.fit(X_train, y_train)

# Предсказание с использованием Ridge модели
y_predict_ridge = ridge.predict(X_test)

print(f'MAE: {mean_absolute_error(y_test, y_predict_ridge)}')
print(f'MSE: {mean_squared_error(y_test, y_predict_ridge)}')
print(f'RMSE: {sqrt(mean_squared_error(y_test, y_predict_ridge))}')
print(f'MAPE: {sqrt(mean_absolute_percentage_error(y_predict_ridge, y_predict_ridge))}')
print(f'R^2: {ridge.score(X_test, y_test)}')

"""После применения метода регуляризации, основные показатели сильно не изменились, идем дальше.

Попробуем выполнить подбор параметра с помощью GridSearchCV, RandomizedSearchCV
"""

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
import numpy as np

param_grid_ridge = {
    'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]
}


ridge_optimal = GridSearchCV(Ridge(), param_grid_ridge).fit(X_train, y_train)
# выводим оптимальные значения параметров


ridge_optimal_rand = RandomizedSearchCV(Ridge(), param_grid_ridge).fit(X_train, y_train)
# выводим оптимальные значения параметров

print(f"RIDGE\nGrindSearch ---> {ridge_optimal.best_params_}\nRandomizedSearch ---> {ridge_optimal_rand.best_params_['alpha']}")



ridge_optimal = GridSearchCV(Lasso(), param_grid_ridge).fit(X_train, y_train)
# выводим оптимальные значения параметров


ridge_optimal_rand = RandomizedSearchCV(Lasso(), param_grid_ridge).fit(X_train, y_train)
# выводим оптимальные значения параметров


print(f"LASSO\nGrindSearch ---> {ridge_optimal.best_params_}\nRandomizedSearch ---> {ridge_optimal_rand.best_params_['alpha']}")

"""Мы нашли самые подходящие гиперпараметры для каждого метода. Теперь применим их на всех предыдущих методах."""

#L1
# Создание и обучение Lasso модели
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# Предсказание с использованием Lasso модели
y_predict_lasso = lasso.predict(X_test)


print('LASSO WITH NEW VALUE')
print(f'MAE: {mean_absolute_error(y_test, y_predict_lasso)}')
print(f'MSE: {mean_squared_error(y_test, y_predict_lasso)}')
print(f'RMSE: {sqrt(mean_squared_error(y_test, y_predict_lasso))}')
print(f'MAPE: {sqrt(mean_absolute_percentage_error(y_predict_lasso, y_predict_lasso))}')
print(f'R^2: {lasso.score(X_test, y_test)}')
print('--------------')


#L2
# Создание и обучение Ridge модели
ridge = Ridge(alpha=1)
ridge.fit(X_train, y_train)

# Предсказание с использованием Ridge модели
y_predict_ridge = ridge.predict(X_test)


print('RIDGE WITH NEW VALUE')
print(f'MAE: {mean_absolute_error(y_test, y_predict_ridge)}')
print(f'MSE: {mean_squared_error(y_test, y_predict_ridge)}')
print(f'RMSE: {sqrt(mean_squared_error(y_test, y_predict_ridge))}')
print(f'MAPE: {sqrt(mean_absolute_percentage_error(y_predict_ridge, y_predict_ridge))}')
print(f'R^2: {ridge.score(X_test, y_test)}')

"""Основные параметры не поменялись.

Применим метод полиномиальной регрессии.
"""

import numpy as np
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline

#применение с методом Ridge
pipeline = make_pipeline(PolynomialFeatures(2), Ridge())
pipeline.fit(X_train, y_train)

pred = pipeline.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test, pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train, y_train)
print('Model determination: ', round(score, 2))

print (f'MSE: {round (mean_squared_error(y_test, pred), 2)}')
print (f'RMSE: {(round (sqrt (mean_squared_error(y_test, pred)),2))}')
print (f'MAPE: {round (sqrt (mean_absolute_percentage_error(y_test, pred)),2)}')
print(f'R^2: {round (pipeline.score (X_test, y_test), 2)}')

"""Метод полиномиальной регрессии показал на данный момент лучшие результаты.
Показатель R^2 = 78%, что на 10% выше предыдущего рекорда, также показатель MSE уменьшился, следовательно количество ошибок в прогнозах сократилось.
"""

#применение с методом Lasso
pipeline = make_pipeline(PolynomialFeatures(2), Lasso())
pipeline.fit(X_train, y_train)

pred = pipeline.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test, pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train, y_train)
print('Model determination: ', round(score, 2))

print (f'MSE: {round (mean_squared_error(y_test, pred), 2)}')
print (f'RMSE: {(round (sqrt (mean_squared_error(y_test, pred)),2))}')
print (f'MAPE: {round (sqrt (mean_absolute_percentage_error(y_test, pred)),2)}')
print(f'R^2: {round (pipeline.score (X_test, y_test), 2)}')

#применение с методом Ridge
pipeline = make_pipeline(PolynomialFeatures(2), Ridge())
pipeline.fit(X_train, y_train)

pred = pipeline.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test, pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train, y_train)
print('Model determination: ', round(score, 2))

print (f'MSE: {round (mean_squared_error(y_test, pred), 2)}')
print (f'RMSE: {(round (sqrt (mean_squared_error(y_test, pred)),2))}')
print (f'MAPE: {round (sqrt (mean_absolute_percentage_error(y_test, pred)),2)}')
print(f'R^2: {round (pipeline.score (X_test, y_test), 2)}')

"""Вместе с методом Lasso полиномиальная регрессия сработала неплохо, но есть результаты выше.
В то время, как вместе с Ridge показатели схожи с рекордными.

# Итог

Как итог, имеем двух финалистов с самыми высокими показателями точности предсказаний, а именно **метод Полиномиальной регрессии** и этот же метод в совокупности с **методом Ridge**.

Ниже будут приведены лучшие показатели:

```
Mean error: 2.41e+03 (32.2%)
Model determination:  0.77
MSE: 5803490.83
RMSE: 2409.04
MAPE: 0.63
R^2: 0.78
```
"""